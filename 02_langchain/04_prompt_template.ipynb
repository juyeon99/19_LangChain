{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_template() ë©”ì†Œë“œ\n",
    "* ë³€ìˆ˜ë¥¼ ì¤‘ê´„í˜¸ë¡œ ë¬¶ì–´ì„œ í…œí”Œë¦¿ì— ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['name'] input_types={} partial_variables={} template='{name}ì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# í…œí”Œë¦¿ ì •ì˜\n",
    "# {} ì•ˆì˜ ë‚´ìš©ì€ ì´í›„ì— ê°’ì´ ë“¤ì–´ê°ˆ ìë¦¬\n",
    "template = \"{name}ì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "# from_template()ì„ ì‚¬ìš©í•´ PromptTemplate ê°ì²´ë¥¼ ìƒì„±\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bearì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "# prompt ìƒì„± (ì™„ì„±) format() ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ë³€ìˆ˜ì— ê°’ì„ ë„£ìŒ\n",
    "prompt = prompt.format(name=\"bear\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonì€ ê·€ë„ ë°˜ ë¡œì¸(Guido van Rossum)ì— ì˜í•´ 1989ë…„ì— ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ì´ ì–¸ì–´ë¥¼ ë§Œë“¤ ë•Œ ì½”ë“œì˜ ê°€ë…ì„±ì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ì˜€ê³ , ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ë²•ì„ ì§€í–¥í–ˆìŠµë‹ˆë‹¤. Pythonì€ 1991ë…„ì— ì²« ë²ˆì§¸ ë²„ì „ì´ ê³µê°œë˜ì—ˆìœ¼ë©°, ì´í›„ë¡œ ë§ì€ ê°œë°œìë“¤ì— ì˜í•´ ë°œì „í•´ì™”ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ì ìœ¼ë¡œëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€ìˆ˜ë¥¼ ì‘ì„±í•´ì•¼ í•˜ì§€ë§Œ ë³€ìˆ˜ê°€ í•œê°œì¼ë•ŒëŠ” ë³€ìˆ˜ë§Œ ì‘ì„± ê°€ëŠ¥\n",
    "\n",
    "# print(chain.invoke({\"language\":\"Python\"}).content)\n",
    "print(chain.invoke(\"Python\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate ê°ì²´ ìƒì„±ê³¼ ë™ì‹œì— prompt ìƒì„±\n",
    "* input_variables ì¸ìë¥¼ ì‚¬ìš©í•´ ë³€ìˆ˜ ì§€ì •\n",
    "* í…œí”Œë¦¿ì— ì‘ì„±í•œ ë³€ìˆ˜ê°€ input_variablesì— ì—†ìœ¼ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PythonëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template ì •ì˜\n",
    "template = \"{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"language\"]\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "prompt.format(language=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partial_variables**\n",
    "* ì—°ì‚° ì¤‘ ë¯¸ë¦¬ ê³„ì‚°ëœ ë³€ìˆ˜ë¥¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì§€ì •í•´ ë„£ì„ ìˆ˜ ìˆë‹¤.\n",
    "* í•­ìƒ ê³µí†µëœ ë°©ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê³  ì‹¶ì€ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‚¬ìš©<br>\n",
    "ex) ë‚ ì§œ, ì‹œê°„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language1'] input_types={} partial_variables={'language2': 'JAVA'} template='{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "# template ì •ì˜\n",
    "template = \"{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "# PromptTemplate ê°ì²´ ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"language1\"],\n",
    "    partial_variables={\n",
    "        \"language2\":\"JAVA\"  # dictionary í˜•íƒœë¡œ partial_variable ì‘ì„±\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pythonê³¼ JAVAëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(language1=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partial()**\n",
    "* ì—°ì‚°ì¤‘ì— ë¯¸ë¦¬ ê³„ì‚°ëœ ë³€ìˆ˜ë¥¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì§€ì •í•´ì„œ ë„£ì„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language1'] input_types={} partial_variables={'language2': 'JavaScript'} template='{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "prompt_partial = prompt.partial(language2=\"JavaScript\")\n",
    "\n",
    "print(prompt_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Pythonê³¼ JavaScriptëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_partial.invoke(\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonì€ ê·€ë„ ë°˜ ë¡œì¸(Guido van Rossum)ì— ì˜í•´ 1991ë…„ì— ì²˜ìŒ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” Pythonì„ ë§Œë“¤ ë•Œ ì½”ë“œì˜ ê°€ë…ì„±ì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ì˜€ê³ , ì´ë¥¼ í†µí•´ í”„ë¡œê·¸ë˜ë°ì´ ë” ì‰½ê³  ì¦ê±°ìš´ ê²½í—˜ì´ ë˜ê¸°ë¥¼ ì›í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "JavaScriptëŠ” ë¸Œë Œë˜ ì•„ì´í¬(Brendan Eich)ì— ì˜í•´ 1995ë…„ì— ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ë„·ìŠ¤ì¼€ì´í”„(Netscape)ì—ì„œ ì¼í•˜ë©´ì„œ ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ë™ì ì¸ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê¸° ìœ„í•´ JavaScriptë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” \"Mocha\"ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì‹œì‘í–ˆì§€ë§Œ, ë‚˜ì¤‘ì— \"LiveScript\"ë¡œ ë³€ê²½ë˜ì—ˆê³ , ìµœì¢…ì ìœ¼ë¡œ \"JavaScript\"ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—¬ì¡ŒìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_partial | llm\n",
    "\n",
    "print(chain.invoke(\"Python\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì¼ë¡œë¶€í„° template ì½ì–´ì˜¤ê¸°\n",
    "* promptë¥¼ í¸í•˜ê²Œ ì‘ì„± ë° ìˆ˜ì • ê°€ëŠ¥\n",
    "* ìƒí™©ì— ë§ê²Œ íŒŒì¼ì„ ì‘ì„±í•´ë‘ë©´ í•„ìš”í•  ë•Œë§ˆë‹¤ êº¼ë‚´ì„œ ì“¸ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language} ì–¸ì–´ì— ëŒ€í•´ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "# prompt = load_prompt(\"ê²½ë¡œ\", encoding=\"utf-8\")\n",
    "prompt = load_prompt(\"prompts/language_simple.yaml\", encoding=\"utf-8\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python ì–¸ì–´ì— ëŒ€í•´ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(language=\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ì–¸ì–´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
      "ì–¸ì–´ì˜ íŠ¹ì§•ì„ ë‹¤ìŒì˜ ì–‘ì‹ì— ë§ê²Œ ì •ë¦¬í•˜ì„¸ìš”.\n",
      "300ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
      "í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
      "---\n",
      "#ì–‘ì‹\n",
      "1. íŠ¹ì§•\n",
      "2. ì œì‘ì\n",
      "3. ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬\n",
      "4. ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = load_prompt(\"prompts/language.yaml\", encoding=\"utf-8\")\n",
    "print(prompt2.format(language=\"Python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. íŠ¹ì§•: Pythonì€ ê°„ê²°í•˜ê³  ê°€ë…ì„±ì´ ë†’ì€ ë¬¸ë²•ì„ ê°€ì§„ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„(ê°ì²´ì§€í–¥, í•¨ìˆ˜í˜• ë“±)ì„ ì§€ì›í•©ë‹ˆë‹¤. ë™ì  íƒ€ì´í•‘ê³¼ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ì œê³µí•˜ì—¬ ê°œë°œ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\n",
      "\n",
      "2. ì œì‘ì: Pythonì€ ê·€ë„ ë°˜ ë¡œì„¬(Guido van Rossum)ì— ì˜í•´ 1991ë…„ì— ì²˜ìŒ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬: Django, Flask, FastAPI ë“± ë‹¤ì–‘í•œ ì›¹ í”„ë ˆì„ì›Œí¬ê°€ ìˆìœ¼ë©°, ë°ì´í„° ê³¼í•™ ë¶„ì•¼ì—ì„œëŠ” Pandas, NumPy, TensorFlow ë“±ì´ ìœ ëª…í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼: ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, ì¸ê³µì§€ëŠ¥, ë¨¸ì‹ ëŸ¬ë‹, ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt2 | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0) | StrOutputParser()\n",
    "\n",
    "answer = chain.invoke(\"Python\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "* ëŒ€í™” ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì…í•˜ê³ ì í•  ë•Œ ì‚¬ìš© ê°€ëŠ¥\n",
    "* ë©”ì„¸ì§€ëŠ” íŠœí”Œ í˜•íƒœë¡œ ì „ë‹¬\n",
    "    * (\"role\", \"message\")ë¡œ êµ¬ì„±\n",
    "    * ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ìƒì„± ê°€ëŠ¥\n",
    "\n",
    "**role**\n",
    "* system : ì‹œìŠ¤í…œ ì„¤ì • ë©”ì„¸ì§€ë¡œ ì£¼ë¡œ ì „ì—­ ì„¤ì •ì„ í•  ë•Œ ì‚¬ìš©\n",
    "* human: ì‚¬ìš©ìì˜ ì…ë ¥ ë©”ì„¸ì§€\n",
    "* ai: AIì˜ ë‹µë³€ ë©”ì„¸ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='{language}ì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"{language}ì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\")\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Pythonì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt.format(language = \"Python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ nonameì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë°˜ê°€ì›Œìš”!', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ {name}ì…ë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"ë°˜ê°€ì›Œìš”!\"),\n",
    "        (\"ai\", \"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\"),\n",
    "        (\"human\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    name=\"noname\", user_input=\"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\"\n",
    ")\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œ ì´ë¦„ì€ nonameì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ëŸ¬ë¶„ì´ ì›í•˜ì‹œëŠ” ëŒ€ë¡œ ë¶ˆëŸ¬ì£¼ì…”ë„ ê´œì°®ìŠµë‹ˆë‹¤! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MessagePlaceHolder\n",
    "* ì•„ì§ í™•ì •ëœ ë©”ì„¸ì§€ê°€ ì•„ë‹ˆì§€ë§Œ, ë‚˜ì¤‘ì— ì±„ì›Œì§ˆ ë©”ì„¸ì§€ ìœ„ì¹˜ë¥¼ ì¡ì•„ë‘ê¸° ìœ„í•´ ì‚¬ìš©\n",
    "* ë³´í†µ ëŒ€í™” ê¸°ë¡ì„ í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['conversation', 'word_count'] input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000024B4B728C20>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ {word_count}ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "        (\"human\", \"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ {word_count}ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.\n",
      "Human: íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\n",
      "AI: ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\n",
      "Human: set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\n",
      "AI: ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\n",
      "Human: ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ 5ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format(\n",
    "    word_count=5,\n",
    "    conversation=[\n",
    "        (\"human\", \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\"),\n",
    "        (\"ai\", \"ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\"),\n",
    "        (\"ai\", \"ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'íŒŒì´ì¬, ë¦¬ìŠ¤íŠ¸, ì¤‘ë³µ ì œê±°, set.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"word_count\":5,\n",
    "    \"conversation\":[\n",
    "        (\"human\", \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\"),\n",
    "        (\"ai\", \"ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\"),\n",
    "        (\"ai\", \"ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ì•½ 24ì‹œê°„ì…ë‹ˆë‹¤. ì´ë¥¼ í•˜ë£¨ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.\\n\\n\\nìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:\\n\\n* **ì‹œì°¨:**  ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ì‹¤ì œë¡œ 23ì‹œê°„ 56ë¶„ìœ¼ë¡œ, ì´ê²ƒì´ **ë³¸ì§ˆì ì¸ ìì „ ì‹œê°„**ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” \"í•˜ë£¨\"ë¼ëŠ” ë‹¨ìœ„ëŠ” 24ì‹œê°„ì´ë©°, 1ì¼ì—ì„œ ì‹¤ì œ ìì „ ì‹œê°„ê³¼ì˜ ì°¨ì´ëŠ” ê³ ë ¤í•˜ì—¬ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.  \\n\\n* **íƒœì–‘ ê¸°ì¤€:** ì§€êµ¬ì˜ ìì „ì€ íƒœì–‘ì„ ì¤‘ì‹¬ìœ¼ë¡œ íšŒì „í•˜ë©° ì´ë¥¼ \\'ìì „\\'ì´ë¼ê³  í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ë¥¸ í–‰ì„±ì´ë‚˜ ì²œì²´ì™€ ê°™ì€ ë‹¤ë¥¸ ê´€ì ì—ì„œ ë³¸ ê²½ìš°, ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ë‹¤ë¥´ê²Œ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\n\\n* **ìˆ˜ì •:**  ì§€êµ¬ì˜ ìì „ ì†ë„ëŠ” ì‹œê°„ì— ë”°ë¼ ë³€í™”í•©ë‹ˆë‹¤. íƒœì–‘ê³¼ ë‹¬ì˜ ì¤‘ë ¥ì´ ì˜í–¥ì„ ë¯¸ì¹˜ë©°, ì§€ê°íŒ ìš´ë™ ë˜í•œ ìì „ ì†ë„ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\\n\\n\\n ë” ë§ì€ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# model\n",
    "llm = Ollama(model=\"gemma2:9b\")\n",
    "\n",
    "response = llm.invoke(\"ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ”?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gemma2:9b\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'íŒŒì´ì¬, ì¤‘ë³µ ì œê±°, set ì‚¬ìš©, ê°„ë‹¨ í•´ê²°, ìœ ìš©  \\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

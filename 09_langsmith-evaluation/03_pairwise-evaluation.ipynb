{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Evaluation\n",
    "* ë‘ ê°œ ì´ìƒì˜ LLM ìƒì„±ë¬¼ì„ ì„œë¡œ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def evaluate_pairwise(runs: list, example) -> dict:\n",
    "    # ì ìˆ˜ ì €ì¥\n",
    "    scores = {}\n",
    "    for i, run in enumerate(runs):\n",
    "        scores[run.id] = i\n",
    "    \n",
    "    # ê° ì˜ˆì œì— ëŒ€í•œ ì‹¤í–‰ ìŒ\n",
    "    answer_a = runs[0].outputs[\"answer\"]\n",
    "    answer_b = runs[1].outputs[\"answer\"]\n",
    "    question = example.inputs[\"question\"]\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    grade_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an LLM judge. Compare the following two answers to a question and determine which one is better.\n",
    "        Better answer is the one that is more detailed and informative.\n",
    "        If the answer is not related to the question, it is not a good answer.\n",
    "\n",
    "        \n",
    "        #Question:\n",
    "        {question}\n",
    "        \n",
    "        #Answer A: \n",
    "        {answer_a}\n",
    "        \n",
    "        #Answer B: \n",
    "        {answer_b}\n",
    "        \n",
    "        Output should be either `A` or `B`. Pick the answer that is better.\n",
    "        \n",
    "        #Preference:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    answer_grader = grade_prompt | llm | StrOutputParser()\n",
    "\n",
    "    score = answer_grader.invoke(\n",
    "        {\n",
    "            \"question\":question,\n",
    "            \"answer_a\":answer_a,\n",
    "            \"answer_b\":answer_b\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if score == \"A\":    # Aê°€ ë‹µë³€ì„ ë” ì˜í–ˆë‹¤.\n",
    "        scores[runs[0].id] = 1\n",
    "        scores[runs[1].id] = 0\n",
    "    elif score == \"B\":  # Bê°€ ë‹µë³€ì„ ë” ì˜í–ˆë‹¤.\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 1\n",
    "    else:\n",
    "        scores[runs[0].id] = 0\n",
    "        scores[runs[1].id] = 0\n",
    "\n",
    "    return {\"key\":\"ranked_preference\", \"scores\":scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def ask_question_with_llm(llm):\n",
    "    rag = PDFRAG(\n",
    "        \"data/snow-white.pdf\",\n",
    "        llm\n",
    "    )\n",
    "    \n",
    "    retriever = rag.create_retriever()\n",
    "\n",
    "    rag_chain = rag.create_chain(retriever)\n",
    "\n",
    "    def _ask_question(inputs: dict):\n",
    "        context = retriever.invoke(inputs[\"question\"])\n",
    "        context = \"\\n\".join([doc.page_content for doc in context])\n",
    "        return {\n",
    "            \"question\":inputs[\"question\"],\n",
    "            \"context\":context,\n",
    "            \"answer\":rag_chain.invoke(inputs[\"question\"])\n",
    "        }\n",
    "    \n",
    "    return _ask_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b436c3b8-16f3-4256-8743-6d06363eed29-0', usage_metadata={'input_tokens': 13, 'output_tokens': 21, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "gpt3 = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "gpt3.invoke(\"ì•ˆë…•í•˜ì„¸ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹ \\n\\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š', additional_kwargs={}, response_metadata={'model': 'gemma2:9b', 'created_at': '2024-11-04T05:37:47.1582547Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3026605700, 'load_duration': 58629400, 'prompt_eval_count': 13, 'prompt_eval_duration': 302800000, 'eval_count': 19, 'eval_duration': 2663151000}, id='run-c692ecf7-320d-45e4-9075-e4483ca3310a-0', usage_metadata={'input_tokens': 13, 'output_tokens': 19, 'total_tokens': 32})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Ollama ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "ollama = ChatOllama(model=\"gemma2:9b\")\n",
    "\n",
    "# Ollama ëª¨ë¸ í˜¸ì¶œ\n",
    "ollama.invoke(\"ì•ˆë…•í•˜ì„¸ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "gpt3_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0))\n",
    "\n",
    "# Ollama ì‚¬ìš©ì‹œ\n",
    "ollama_chain = ask_question_with_llm(ChatOllama(model=\"gemma2:9b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVALUATION-6e8c530b' at:\n",
      "https://smith.langchain.com/o/0952f302-ca2e-4d37-9a5b-1830d66b833b/datasets/a81bc91f-29c2-4be9-a030-f450d6fd800a/compare?selectedSessions=0625bbee-6422-42ae-ab21-98716c9c6f06\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6f974e52f04fdfa98936937608a3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'MODEL_COMPARE_EVALUATION-6a551a5c' at:\n",
      "https://smith.langchain.com/o/0952f302-ca2e-4d37-9a5b-1830d66b833b/datasets/a81bc91f-29c2-4be9-a030-f450d6fd800a/compare?selectedSessions=4c3dee64-da5e-4b41-838c-563fcf2fee14\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932a67a8a11b485492278045387210d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    config={\"llm\":ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)},  # í‰ê°€ì\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\":run.outputs[\"answer\"],\n",
    "        \"reference\":run.outputs[\"context\"],\n",
    "        \"input\":example.inputs[\"question\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "dataset_name = \"RAG_EVALUATION_DATASET\"\n",
    "\n",
    "experiment_result1 = evaluate(\n",
    "    gpt3_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\":\"GPT-3.5-turbo í‰ê°€ (cot_qa)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "experiment_result2 = evaluate(\n",
    "    gpt4o_chain,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator],\n",
    "    experiment_prefix=\"MODEL_COMPARE_EVALUATION\",\n",
    "    metadata={\n",
    "        \"variant\": \"GPT-4o-mini í‰ê°€ (cot_qa)\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/0952f302-ca2e-4d37-9a5b-1830d66b833b/datasets/a81bc91f-29c2-4be9-a030-f450d6fd800a/compare?selectedSessions=ddd324f6-9545-4196-ab49-7fc887545ef4%2C2841d0b6-4482-4cc6-973c-cbf0b6ae4412&comparativeExperiment=841b8b9c-041d-41f5-8f3d-192eba40657b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c20e89ff9441efadd719d8c9ae8df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x2d65e57bc50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate_comparative\n",
    "\n",
    "evaluate_comparative(\n",
    "    [\"MODEL_COMPARE_EVALUATION-393290e4\",\"MODEL_COMPARE_EVALUATION-d0667ace\"],\n",
    "    # í‰ê°€ì\n",
    "    evaluators=[evaluate_pairwise]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
